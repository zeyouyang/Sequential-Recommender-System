{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a287c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import (SnowballStemmer)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = torch.load('C://Users//user//Desktop//王茂田//item.pt')\n",
    "duration=torch.load('C://Users//user//Desktop//王茂田//duration.pt')\n",
    "interval=torch.load('C://Users//user//Desktop//王茂田//interval.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c09057",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = item.numpy().tolist()\n",
    "duration = duration.numpy().tolist()\n",
    "interval = interval.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_item=[]\n",
    "target_duration=[]\n",
    "target_interval=[]\n",
    "\n",
    "for i in range (len(item)):\n",
    "    target_item=target_item+(item[i][-1:])\n",
    "    \n",
    "for i in range (len(duration)):\n",
    "    target_duration=target_duration+(duration[i][-1:])\n",
    "    \n",
    "for i in range (len(interval)):\n",
    "    target_interval=target_interval+(interval[i][-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba971a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_item=[]\n",
    "input_duration=[]\n",
    "input_interval=[]\n",
    "\n",
    "for i in range (len(item)):\n",
    "    input_item.append(item[i][:-1])\n",
    "    \n",
    "for i in range (len(duration)):\n",
    "    input_duration.append(duration[i][:-1])\n",
    "    \n",
    "for i in range (len(interval)):\n",
    "    input_interval.append(interval[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5660189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_lists1=(input_item,target_item,input_duration,target_duration,input_interval,target_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l1=random.sample(range(0,len(lists_of_lists1[0])),int(len(lists_of_lists1[0])*0.7))\n",
    "train_data=[[],[],[],[],[],[],[],[]]\n",
    "test_data=[[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(lists_of_lists1[0])):\n",
    "    if i in set(l1):\n",
    "        train_data[0].append(lists_of_lists1[0][i])\n",
    "        train_data[1].append(lists_of_lists1[1][i])\n",
    "        train_data[2].append(lists_of_lists1[2][i])\n",
    "        train_data[3].append(lists_of_lists1[3][i])\n",
    "        train_data[4].append(lists_of_lists1[4][i])\n",
    "        train_data[5].append(lists_of_lists1[5][i])\n",
    "        train_data[6].append(len(lists_of_lists1[0][i]))\n",
    "        train_data[7].append(i)\n",
    "    else:\n",
    "        test_data[0].append(lists_of_lists1[0][i])\n",
    "        test_data[1].append(lists_of_lists1[1][i])\n",
    "        test_data[2].append(lists_of_lists1[2][i])\n",
    "        test_data[3].append(lists_of_lists1[3][i])\n",
    "        test_data[4].append(lists_of_lists1[4][i])\n",
    "        test_data[5].append(lists_of_lists1[5][i])\n",
    "        test_data[6].append(len(lists_of_lists1[0][i]))\n",
    "        test_data[7].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd01fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0] = torch.LongTensor(train_data[0]).to(torch.int64)\n",
    "train_data[1] = torch.LongTensor(train_data[1]).to(torch.int64)\n",
    "train_data[2] = torch.LongTensor(train_data[2]).to(torch.int64)\n",
    "train_data[3] = torch.LongTensor(train_data[3]).to(torch.int64)\n",
    "train_data[4] = torch.LongTensor(train_data[4]).to(torch.int64)\n",
    "train_data[5] = torch.LongTensor(train_data[5]).to(torch.int64)\n",
    "train_data[6] = torch.LongTensor(train_data[6]).to(torch.int64)\n",
    "train_data[7] = torch.LongTensor(train_data[7]).to(torch.int64)\n",
    "\n",
    "test_data[0] = torch.LongTensor(test_data[0]).to(torch.int64)\n",
    "test_data[1] = torch.LongTensor(test_data[1]).to(torch.int64)\n",
    "test_data[2] = torch.LongTensor(test_data[2]).to(torch.int64)\n",
    "test_data[3] = torch.LongTensor(test_data[3]).to(torch.int64)\n",
    "test_data[4] = torch.LongTensor(test_data[4]).to(torch.int64)\n",
    "test_data[5] = torch.LongTensor(test_data[5]).to(torch.int64)\n",
    "test_data[6] = torch.LongTensor(test_data[6]).to(torch.int64)\n",
    "test_data[7] = torch.LongTensor(test_data[7]).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15855b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5004d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "train = data_utils.TensorDataset(train_data[0], train_data[1],train_data[2],train_data[3], train_data[4],train_data[5],train_data[6],train_data[7])\n",
    "test = data_utils.TensorDataset(test_data[0],test_data[1], test_data[2],test_data[3],test_data[4], test_data[5], test_data[6], test_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入dataloader，並選擇batch_size\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "a = dataiter.next()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9039056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caser(nn.Module):\n",
    "    # CASER 主架構 不用動\n",
    "    def __init__(self, user_num, his_max, hidden_size = 128, emb_size = 128, num_horizon = 128, num_vertical = 4, L = 9):\n",
    "        super(Caser, self).__init__()\n",
    "      \n",
    "        self.user_num = user_num\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_his = his_max\n",
    "        self.num_horizon = num_horizon\n",
    "        self.num_vertical = num_vertical\n",
    "        self.l = L\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.u_embeddings = nn.Embedding(self.user_num + 1, self.emb_size, padding_idx=0)\n",
    "        self.it_embedding = nn.Embedding(776, self.emb_size)\n",
    "        self.du_embedding = nn.Embedding(21, self.emb_size)\n",
    "        self.int_embedding = nn.Embedding(21, self.emb_size)\n",
    "        lengths = [i + 1 for i in range(self.l)]\n",
    "        self.conv_h = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels=1, out_channels=self.num_horizon, kernel_size=(i, self.emb_size*3)) for i in lengths])\n",
    "        self.conv_v = nn.Conv2d(in_channels=1, out_channels=self.num_vertical, kernel_size=(self.max_his, 1))\n",
    "\n",
    "        self.fc_dim_h = self.num_horizon * len(lengths)\n",
    "        self.fc_dim_v = self.num_vertical * self.emb_size*3\n",
    "        fc_dim_in = self.fc_dim_v + self.fc_dim_h\n",
    "        self.fc = nn.Linear(fc_dim_in, self.emb_size*6)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.out_it = nn.Linear(self.hidden_size*7, 776)\n",
    "        self.out_du = nn.Linear(self.hidden_size*7, 21)\n",
    "        self.out_int = nn.Linear(self.hidden_size*7, 21)\n",
    "\n",
    "    \n",
    "    def get_his_max_tensor(self, his_tensor, lengths, his_max):\n",
    "        his_max_list = []\n",
    "        for idx,l in enumerate(lengths):\n",
    "            if int(l) > his_max:\n",
    "                his_max_list.append(his_tensor[idx,int(l)-his_max:int(l)].tolist())\n",
    "            else:\n",
    "                his_max_list.append(his_tensor[idx,0:his_max].tolist())\n",
    "\n",
    "        his_max_tensor = torch.tensor(his_max_list).to(self.device)\n",
    "\n",
    "        his_max_length = torch.tensor([len(t[t!=0]) for t in his_max_tensor]).to(self.device)\n",
    "        return his_max_tensor, his_max_length\n",
    "    \n",
    "    def get_length_tensor(self, tensor):\n",
    "        return torch.tensor([len(t[t!=0]) for t in tensor])\n",
    "    \n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        items= input['item']\n",
    "        target_items = input['target_item']\n",
    "        durations = input['duration'] \n",
    "        target_durations = input['target_duration']\n",
    "        intervals = input['interval'] \n",
    "\n",
    "      \n",
    "        his_length = input['lengths']  \n",
    "\n",
    "        u_ids = input['user']\n",
    "\n",
    "        items, his_length = self.get_his_max_tensor(items, his_length, self.max_his)\n",
    "        durations, his_length1 = self.get_his_max_tensor(durations, his_length, self.max_his)\n",
    "        intervals, his_length2 = self.get_his_max_tensor(intervals, his_length, self.max_his)\n",
    "        items = items.to(self.device)\n",
    "        durations = durations.to(self.device)\n",
    "        intervals = intervals.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        embedded_item = self.it_embedding(items)\n",
    "   \n",
    "        embedded_duration = self.du_embedding(durations)\n",
    "        embedded_interval = self.int_embedding(intervals)\n",
    "        his_vectors = torch.cat((embedded_item,embedded_duration,embedded_interval),dim=2)\n",
    "\n",
    "        his_vectors = his_vectors.unsqueeze(1)  # [batch_size, 1, history_max, emb_size]\n",
    "\n",
    "        # Convolution Layers\n",
    "        out, out_h, out_v = None, None, None\n",
    "        # vertical conv layer\n",
    "        if self.num_vertical > 0:\n",
    "            out_v = self.conv_v(his_vectors)\n",
    "            out_v = out_v.view(-1, self.fc_dim_v)  # prepare for fully connect\n",
    "        # horizontal conv layer\n",
    "        out_hs = list()\n",
    "        if self.num_horizon > 0:\n",
    "            for conv in self.conv_h:\n",
    "                conv_out = conv(his_vectors).squeeze(3).relu()\n",
    "                pool_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "                out_hs.append(pool_out)\n",
    "            out_h = torch.cat(out_hs, 1)  # prepare for fully connect\n",
    "\n",
    "        # Fully-connected Layers\n",
    "        user_vector = self.u_embeddings(u_ids)\n",
    "        out_all = torch.cat([out_v, out_h], 1)\n",
    "        out_all = self.dropout(out_all)\n",
    "        z = self.fc(out_all).relu()\n",
    "       \n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        output1 = self.out_it(torch.cat([z, user_vector], 1))\n",
    "        output2 = self.out_du(torch.cat([z, user_vector], 1))\n",
    "        output3 = self.out_int(torch.cat([z, user_vector], 1))\n",
    "\n",
    "        return output1,output2,output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29baf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num, his_max = 2500, 99\n",
    "model = Caser(user_num, his_max)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, betas=(0.9, 0.99), eps=1e-8)\n",
    "epochs = 30\n",
    "model = model.to(device)\n",
    "train_losses = []\n",
    "\n",
    "logs = []\n",
    "prod_all = []\n",
    "label_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581063c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da892eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for epoch in range(epochs):\n",
    "    co = 0\n",
    "\n",
    "    correct_duration=0\n",
    "    correct_interval=0\n",
    "  \n",
    "    train_loss_sum = 0.0\n",
    "    \n",
    "    train_loss = list()\n",
    "\n",
    "    start_time = timer()\n",
    "    model.train()\n",
    "   \n",
    "    b = []\n",
    "    m, t= 0,0\n",
    "    re3_it, re5_it, re10_it,  ra_it = 0, 0, 0, 0\n",
    "    re2_du, re3_du, re5_du,  ra_du = 0, 0, 0, 0\n",
    "    re2_int, re3_int, re5_int,  ra_int = 0, 0, 0, 0\n",
    "    m3_it,m5_it,m10_it=0,0,0\n",
    "    nd3_it,nd5_it,nd10_it=0,0,0\n",
    "    \n",
    "    m2_du,m3_du,m5_du=0,0,0\n",
    "    nd2_du,nd3_du,nd5_du=0,0,0\n",
    "    \n",
    "    m2_int,m3_int,m5_int=0,0,0\n",
    "    nd2_int,nd3_int,nd5_int=0,0,0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch, data in enumerate(train_loader):\n",
    "        #將input的tuple分成三個部分\n",
    "        \n",
    "        input = { 'item': data[0].to(device),\n",
    "                  'target_item': data[1].to(device),\n",
    "                  'duration': data[2].to(device),\n",
    "                  'target_duration': data[3].to(device),\n",
    "                  'interval': data[4].to(device),\n",
    "                  'target_interval': data[5].to(device),\n",
    "                  'lengths': data[6].to(device),\n",
    "                  'user': data[7].to(device),\n",
    "                 }\n",
    "        logits_it,logits_du,logits_int = model(input)\n",
    "        l_it = logits_it\n",
    "        l_du = logits_du\n",
    "        l_int = logits_int\n",
    "        tgt_it = input['target_item']\n",
    "        tgt_du = input['target_duration']\n",
    "        tgt_int = input['target_interval']\n",
    "        \n",
    "       \n",
    "        # CROSS比較train跟target\n",
    "        loss_it = loss_fn(l_it, tgt_it)\n",
    "        loss_du = loss_fn(l_du, tgt_du)\n",
    "        loss_int = loss_fn(l_int, tgt_int)\n",
    "        \n",
    "        loss = 0.4*loss_it + 0.3*loss_du +0.3*loss_int\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.item()\n",
    "       \n",
    "        \n",
    "        \n",
    " \n",
    "    train_loss = train_loss_sum / len(train_loader)\n",
    "    train_time = round(timer() - start_time)\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    import heapq\n",
    "    \n",
    "   \n",
    "\n",
    "    for batch, data in tqdm(enumerate(test_loader)):   \n",
    "        input = { 'item': data[0].to(device),\n",
    "                  'target_item': data[1].to(device),\n",
    "                  'duration': data[2].to(device),\n",
    "                  'target_duration': data[3].to(device),\n",
    "                  'interval': data[4].to(device),\n",
    "                  'target_interval': data[5].to(device),\n",
    "                  'lengths': data[6].to(device),\n",
    "                  'user': data[7].to(device),\n",
    "                 }\n",
    "\n",
    "        logits_it,logits_du,logits_int = model(input)\n",
    "   \n",
    "        l_it = logits_it\n",
    "        l_du = logits_du\n",
    "        l_int = logits_int\n",
    "        tgt_it = input['target_item']\n",
    "        tgt_du = input['target_duration']\n",
    "        tgt_int = input['target_interval']\n",
    "\n",
    "        \n",
    "        l_it =  l_it.cpu().data.numpy()\n",
    "        l_du =  l_du.cpu().data.numpy()\n",
    "        l_int =  l_int.cpu().data.numpy()\n",
    "        ranks_it = 0\n",
    "        ranks3_it = 0\n",
    "        ranks5_it = 0\n",
    "        ranks10_it = 0\n",
    "        ndcg3_it=0\n",
    "        ndcg5_it=0\n",
    "        ndcg10_it=0\n",
    "        \n",
    "        ranks_du = 0\n",
    "        ranks2_du = 0\n",
    "        ranks3_du = 0\n",
    "        ranks5_du = 0\n",
    "        ndcg2_du=0\n",
    "        ndcg3_du=0\n",
    "        ndcg5_du=0\n",
    "        \n",
    "        ranks_int = 0\n",
    "        ranks2_int = 0\n",
    "        ranks3_int = 0\n",
    "        ranks5_int = 0\n",
    "        ndcg2_int=0\n",
    "        ndcg3_int=0\n",
    "        ndcg5_int=0\n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "       #####item\n",
    "        # 將前k個結果取出做比較\n",
    "        for n in range(len(l_it)):\n",
    "             \n",
    "            r3= heapq.nlargest(3, range(len(l_it[n])), l_it[n].take) \n",
    "\n",
    "            \n",
    "            for i in range(len(r3)):\n",
    "                re3_it += ((r3[i]) == tgt_it[n]).float().sum()\n",
    "                \n",
    "            r5= heapq.nlargest(5, range(len(l_it[n])), l_it[n].take)\n",
    "       \n",
    "            for i in range(len(r5)):\n",
    "                re5_it += ((r5[i]) == tgt_it[n]).float().sum()\n",
    "                \n",
    "                \n",
    "            r10= heapq.nlargest(10, range(len(l_it[n])), l_it[n].take)\n",
    "          \n",
    "            for i in range(len(r10)):\n",
    "                re10_it += ((r10[i]) ==tgt_it[n]).float().sum()\n",
    "            \n",
    "          #MRR3\n",
    "            mr3 = r3\n",
    "            for i in range(len(mr3)):\n",
    "                if (mr3[i]) == tgt_it[n]:\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "\n",
    "                    m3_it += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_it+=nndcg3\n",
    "        #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "               \n",
    "                if (mr5[i]) == tgt_it[n]:\n",
    "                    rank5 = i + 1\n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_it += rranks5\n",
    "                    nndcg5=(1/(np.log2(rank5+1)))\n",
    "                    nd5_it+=nndcg5\n",
    "    \n",
    "    \n",
    "    #MRR\n",
    "            mr10 = r10\n",
    "\n",
    "            for i in range(len(mr10)):\n",
    "                if (mr10[i]) == tgt_it[n]:\n",
    "                    rank10 = i + 1\n",
    "                   \n",
    "                    rranks10 = 1/rank10\n",
    "                    m10_it += rranks10\n",
    "                    nndcg10=1/(np.log2(rank10+1))\n",
    "                    nd10_it+=nndcg10\n",
    "        \n",
    "         #####duration\n",
    "        # 將前k個結果取出做比較      \n",
    "        \n",
    "        \n",
    "        for n in range(len(l_du)):\n",
    "            \n",
    "            r1= heapq.nlargest(1,  range(len(l_du[n])), l_du[n].take) \n",
    "            \n",
    "            \n",
    "            for i in range(len(r1)):\n",
    "                correct_duration += (((r1[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r2= heapq.nlargest(2,  range(len(l_du[n])), l_du[n].take) \n",
    "                \n",
    "      \n",
    "            for i in range(len(r2)):\n",
    "                re2_du += (((r2[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r3= heapq.nlargest(3,  range(len(l_du[n])), l_du[n].take) \n",
    "         \n",
    "            for i in range(len(r3)):\n",
    "                re3_du += (((r3[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            r5= heapq.nlargest(5,  range(len(l_du[n])), l_du[n].take) \n",
    "            \n",
    "            for i in range(len(r5)):\n",
    "                re5_du += (((r5[i]) ==tgt_du[n]).float().sum())\n",
    "                \n",
    "           \n",
    "                           \n",
    "          #MRR3\n",
    "            mr2 = r2\n",
    "            for i in range(len(mr2)):\n",
    "                if ((mr2[i]) == tgt_du[n]):\n",
    "                    rank2 = i + 1\n",
    "                    rranks2 = 1/rank2\n",
    "\n",
    "                    m2_du += rranks2\n",
    "                    nndcg2=(1/(np.log2(rank2+1)))\n",
    "                    nd2_du+=nndcg2\n",
    "        #MRR\n",
    "            mr3 = r3\n",
    "\n",
    "            for i in range(len(mr3)):\n",
    "               \n",
    "                if ((mr3[i]) == tgt_du[n]):\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "                    \n",
    "                    m3_du += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_du+=nndcg3\n",
    "    \n",
    "\n",
    "    #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "                if ((mr5[i]) == tgt_du[n]):\n",
    "                    rank5 = i + 1\n",
    "                   \n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_du += rranks5\n",
    "                    nndcg5=1/(np.log2(rank5+1))\n",
    "                    nd5_du+=nndcg5\n",
    "       \n",
    "                    \n",
    "                      \n",
    "       \n",
    "           #####interval\n",
    "        # 將前k個結果取出做比較  \n",
    "        for n in range(len(l_int)):\n",
    "            \n",
    "            \n",
    "            r1= heapq.nlargest(1,  range(len(l_int[n])), l_int[n].take) \n",
    "           \n",
    "            for i in range(len(r1)):\n",
    "                correct_interval += (((r1[i]) == tgt_int[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r2= heapq.nlargest(2,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "            \n",
    "            for i in range(len(r2)):\n",
    "                re2_int += (((r2[i])) == tgt_int[n]).float().sum()\n",
    "                \n",
    "            r3= heapq.nlargest(3,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "            for i in range(len(r3)):\n",
    "                re3_int += (((r3[i])) == tgt_int[n]).float().sum()\n",
    "                \n",
    "            r5= heapq.nlargest(5,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "     \n",
    "            for i in range(len(r5)):\n",
    "                re5_int += (((r5[i])) == tgt_int[n]).float().sum()\n",
    "            \n",
    "          #MRR3\n",
    "            mr2 = r2\n",
    "            for i in range(len(mr2)):\n",
    "                if (((mr2[i])) == tgt_int[n]):\n",
    "                    rank2 = i + 1\n",
    "                    rranks2 = 1/rank2\n",
    "\n",
    "                    m2_int += rranks2\n",
    "                    nndcg2=(1/(np.log2(rank2+1)))\n",
    "                    nd2_int+=nndcg2\n",
    "        #MRR\n",
    "            mr3 = r3\n",
    "\n",
    "            for i in range(len(mr3)):\n",
    "               \n",
    "                if ((mr3[i])) == tgt_int[n]:\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "                    m3_int += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_int+=nndcg3\n",
    "    \n",
    " \n",
    "    #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "                if ((mr5[i])) == tgt_int[n]:\n",
    "                    rank5 = i + 1\n",
    "                   \n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_int += rranks5\n",
    "                    nndcg5 = 1/(np.log2(rank5+1))\n",
    "                    nd5_int+=nndcg5  \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "   \n",
    "    \n",
    "    recall3_it = (re3_it*100)/(len(test_data[0]))\n",
    "    \n",
    "    recall5_it = (re5_it*100)/(len(test_data[0]))\n",
    "    recall10_it = (re10_it*100)/(len(test_data[0]))\n",
    "    mrr3_it = (m3_it*100)/(len(test_data[0]))\n",
    "    mrr5_it = (m5_it*100)/(len(test_data[0]))\n",
    "    mrr10_it = (m10_it*100)/(len(test_data[0]))\n",
    "    n3_it=(nd3_it*100)/(len(test_data[0]))\n",
    "    n5_it=(nd5_it*100)/(len(test_data[0]))\n",
    "    n10_it=(nd10_it*100)/(len(test_data[0]))\n",
    "   \n",
    "        \n",
    "        \n",
    "    recall2_du = (re2_du*100)/(len(test_data[0]))\n",
    "    recall3_du = (re3_du*100)/(len(test_data[0]))\n",
    "    recall5_du = (re5_du*100)/(len(test_data[0]))\n",
    "    mrr2_du = (m2_du*100)/(len(test_data[0]))\n",
    "    mrr3_du = (m3_du*100)/(len(test_data[0]))\n",
    "    mrr5_du = (m5_du*100)/(len(test_data[0]))\n",
    "    n2_du=(nd2_du*100)/(len(test_data[0]))\n",
    "    n3_du=(nd3_du*100)/(len(test_data[0]))\n",
    "    n5_du=(nd5_du*100)/(len(test_data[0]))\n",
    "    \n",
    "        \n",
    "        \n",
    "    recall2_int = (re2_int*100)/(len(test_data[0]))\n",
    "    recall3_int = (re3_int*100)/(len(test_data[0]))\n",
    "    recall5_int = (re5_int*100)/(len(test_data[0]))\n",
    "    mrr2_int = (m2_int*100)/(len(test_data[0]))\n",
    "    mrr3_int = (m3_int*100)/(len(test_data[0]))\n",
    "    mrr5_int = (m5_int*100)/(len(test_data[0]))\n",
    "    n2_int=(nd2_int*100)/(len(test_data[0]))\n",
    "    n3_int=(nd3_int*100)/(len(test_data[0]))\n",
    "    n5_int=(nd5_int*100)/(len(test_data[0]))\n",
    "\n",
    "        \n",
    "    accuracy_du = 100 * correct_duration/(len(test_data[0]))\n",
    "    accuracy_int = 100 * correct_interval/(len(test_data[0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "    log = f\"Accuracy_Duration: {accuracy_du:.5f} Accuracy_Interval: {accuracy_int:.5f} HR3_Item: {recall3_it:.5f} HR5_Item: {recall5_it:.5f} HR10_Item: {recall10_it:.5f}  HR2_du: {recall2_du:.5f} HR3_du: {recall3_du:.5f} HR5_du: {recall5_du:.5f} HR2_int: {recall2_int:.5f} HR3_int: {recall3_int:.5f}HR5_int: {recall5_int:.5f}  MRR3_item: {mrr3_it:.10f}  MRR5_item: {mrr5_it:.10f}  MRR10_item: {mrr10_it:.10f} MRR2_du: {mrr2_du:.10f}  MRR3_du: {mrr3_du:.10f}  MRR5_du: {mrr5_du:.10f} MRR2_int: {mrr2_int:.10f}  MRR3_int: {mrr3_int:.10f}  MRR5_int: {mrr5_int:.10f}  NDCG3_it: {n3_it:.10f}  NDCG5_it: {n5_it:.10f}  NDCG10_it: {n10_it:.10f}  NDCG2_du: {n2_du:.10f}  NDCG3_du: {n3_du:.10f}  NDCG5_du: {n5_du:.10f}  NDCG2_int: {n2_int:.10f}  NDCG3_int: {n3_int:.10f}  NDCG5_int: {n5_int:.10f} \"\n",
    "        \n",
    "    print(\"train_loss_sum:\",train_loss_sum)\n",
    "    print(log)\n",
    "    \n",
    "   # print(m/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d166aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752ff57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130610a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
