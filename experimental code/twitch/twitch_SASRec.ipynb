{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = torch.load('C://Users//user//Desktop//王茂田//twitch_item.pt')\n",
    "duration=torch.load('C://Users//user//Desktop//王茂田//twitch_duration.pt')\n",
    "interval=torch.load('C://Users//user//Desktop//王茂田//twitch_interval.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdde181",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = item.numpy().tolist()\n",
    "duration = duration.numpy().tolist()\n",
    "interval = interval.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c44cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_item=[]\n",
    "target_duration=[]\n",
    "target_interval=[]\n",
    "\n",
    "for i in range (len(item)):\n",
    "    target_item=target_item+(item[i][-1:])\n",
    "    \n",
    "for i in range (len(duration)):\n",
    "    target_duration=target_duration+(duration[i][-1:])\n",
    "    \n",
    "for i in range (len(interval)):\n",
    "    target_interval=target_interval+(interval[i][-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a95482",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_item=[]\n",
    "input_duration=[]\n",
    "input_interval=[]\n",
    "\n",
    "for i in range (len(item)):\n",
    "    input_item.append(item[i][:-1])\n",
    "    \n",
    "for i in range (len(duration)):\n",
    "    input_duration.append(duration[i][:-1])\n",
    "    \n",
    "for i in range (len(interval)):\n",
    "    input_interval.append(interval[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_lists1=(input_item,target_item,input_duration,target_duration,input_interval,target_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a4d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9617cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l1=random.sample(range(0,len(lists_of_lists1[0])),int(len(lists_of_lists1[0])*0.7))\n",
    "train_data=[[],[],[],[],[],[],[]]\n",
    "test_data=[[],[],[],[],[],[],[]]\n",
    "for i in range(len(lists_of_lists1[0])):\n",
    "    if i in set(l1):\n",
    "        train_data[0].append(lists_of_lists1[0][i])\n",
    "        train_data[1].append(lists_of_lists1[1][i])\n",
    "        train_data[2].append(lists_of_lists1[2][i])\n",
    "        train_data[3].append(lists_of_lists1[3][i])\n",
    "        train_data[4].append(lists_of_lists1[4][i])\n",
    "        train_data[5].append(lists_of_lists1[5][i])\n",
    "        train_data[6].append(len(lists_of_lists1[0][i]))\n",
    "    else:\n",
    "        test_data[0].append(lists_of_lists1[0][i])\n",
    "        test_data[1].append(lists_of_lists1[1][i])\n",
    "        test_data[2].append(lists_of_lists1[2][i])\n",
    "        test_data[3].append(lists_of_lists1[3][i])\n",
    "        test_data[4].append(lists_of_lists1[4][i])\n",
    "        test_data[5].append(lists_of_lists1[5][i])\n",
    "        test_data[6].append(len(lists_of_lists1[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0] = torch.LongTensor(train_data[0]).to(torch.int64)\n",
    "train_data[1] = torch.LongTensor(train_data[1]).to(torch.int64)\n",
    "train_data[2] = torch.LongTensor(train_data[2]).to(torch.int64)\n",
    "train_data[3] = torch.LongTensor(train_data[3]).to(torch.int64)\n",
    "train_data[4] = torch.LongTensor(train_data[4]).to(torch.int64)\n",
    "train_data[5] = torch.LongTensor(train_data[5]).to(torch.int64)\n",
    "train_data[6] = torch.LongTensor(train_data[6]).to(torch.int64)\n",
    "\n",
    "test_data[0] = torch.LongTensor(test_data[0]).to(torch.int64)\n",
    "test_data[1] = torch.LongTensor(test_data[1]).to(torch.int64)\n",
    "test_data[2] = torch.LongTensor(test_data[2]).to(torch.int64)\n",
    "test_data[3] = torch.LongTensor(test_data[3]).to(torch.int64)\n",
    "test_data[4] = torch.LongTensor(test_data[4]).to(torch.int64)\n",
    "test_data[5] = torch.LongTensor(test_data[5]).to(torch.int64)\n",
    "test_data[6] = torch.LongTensor(test_data[6]).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "train = data_utils.TensorDataset(train_data[0], train_data[1],train_data[2],train_data[3], train_data[4],train_data[5],train_data[6])\n",
    "test = data_utils.TensorDataset(test_data[0],test_data[1], test_data[2],test_data[3],test_data[4], test_data[5], test_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112835e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入dataloader，並選擇batch_size\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47356d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59667019",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "a = dataiter.next()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336973ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, kq_same=False, bias=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        It has projection layer for getting keys, queries and values. Followed by attention.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.h = n_heads\n",
    "        self.d_k = self.d_model // self.h\n",
    "        self.kq_same = kq_same\n",
    "\n",
    "        if not kq_same:\n",
    "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "    def head_split(self, x):  # get dimensions bs * h * seq_len * d_k\n",
    "        new_x_shape = x.size()[:-1] + (self.h, self.d_k)\n",
    "        return x.view(*new_x_shape).transpose(-2, -3)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        origin_shape = q.size()\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "        if not self.kq_same:\n",
    "            q = self.head_split(self.q_linear(q))\n",
    "        else:\n",
    "            q = self.head_split(self.k_linear(q))\n",
    "        k = self.head_split(self.k_linear(k))\n",
    "        v = self.head_split(self.v_linear(v))\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        output = self.scaled_dot_product_attention(q, k, v, self.d_k, mask)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        output = output.transpose(-2, -3).reshape(origin_shape)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(q, k, v, d_k, mask=None):\n",
    "        \"\"\"\n",
    "        This is called by Multi-head attention object to find the values.\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / d_k ** 0.5  # bs, head, q_len, k_len\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -np.inf)\n",
    "        scores = (scores - scores.max()).softmax(dim=-1)\n",
    "        scores = scores.masked_fill(torch.isnan(scores), 0)\n",
    "        output = torch.matmul(scores, v)  # bs, head, q_len, d_k\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout=0, kq_same=False):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        This is a Basic Block of Transformer. It contains one Multi-head attention object. \n",
    "        Followed by layer norm and position wise feedforward net and dropout layer.\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention Block\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, n_heads, kq_same=kq_same)\n",
    "\n",
    "        # Two layer norm layer and two dropout layer\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, seq, mask=None):\n",
    "        context = self.masked_attn_head(seq, seq, seq, mask)\n",
    "        context = self.layer_norm1(self.dropout1(context) + seq)\n",
    "        output = self.linear1(context).relu()\n",
    "        output = self.linear2(output)\n",
    "        output = self.layer_norm2(self.dropout2(output) + context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,  emb_size, history_max, num_layers, num_heads, item_num,duration_num,interval_num, dropout):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.emb_size = emb_size\n",
    "        self.max_his = history_max\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.item_num = item_num\n",
    "        self.duration_num = duration_num\n",
    "        self.interval_num = interval_num\n",
    "        self.dropout = dropout\n",
    "        history_max = 100\n",
    "        self.len_range = torch.from_numpy(np.arange(self.max_his))\n",
    "        self.len_range = self.len_range.to(device)\n",
    "        self.it_embeddings = nn.Embedding(self.item_num, self.emb_size)\n",
    "        self.du_embeddings = nn.Embedding(self.duration_num, self.emb_size)\n",
    "        self.int_embeddings = nn.Embedding(self.interval_num, self.emb_size)\n",
    "        \n",
    "        \n",
    "        self.out_it = nn.Linear(self.emb_size*3, 16440)\n",
    "        self.out_du = nn.Linear(self.emb_size*3, 95)\n",
    "        self.out_int = nn.Linear(self.emb_size*3, 21)\n",
    "        \n",
    "        self.p_embeddings = nn.Embedding(self.max_his + 1, self.emb_size*3)\n",
    "\n",
    "        self.transformer_block = nn.ModuleList([\n",
    "            TransformerLayer(d_model=self.emb_size*3, d_ff=self.emb_size*3, n_heads=self.num_heads,\n",
    "                                    dropout=self.dropout, kq_same=False)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "       \n",
    "     \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.check_list = []\n",
    "        \n",
    "        items = input['item'] \n",
    "        target_items = input['target_item']\n",
    "        durations = input['duration'] \n",
    "        target_durations = input['target_duration']\n",
    "        intervals = input['interval'] \n",
    "        target_intervals = input['target_interval']\n",
    "        lengths = input['lengths']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_size, seq_len = items.shape\n",
    "\n",
    "        valid_his = (items > 0).long()\n",
    "        \n",
    "        \n",
    "        embedded_item = self.it_embeddings(items)\n",
    "   \n",
    "        embedded_duration = self.du_embeddings(durations)\n",
    "        embedded_interval = self.int_embeddings(intervals)\n",
    "        \n",
    "        \n",
    "        his_vectors = torch.cat((embedded_item,embedded_duration,embedded_interval),dim=2)\n",
    "\n",
    "       \n",
    "        position = (lengths[:, None] - self.len_range[None, :seq_len]) * valid_his\n",
    "        pos_vectors = self.p_embeddings(position)\n",
    "        his_vectors = his_vectors + pos_vectors\n",
    "\n",
    "        # Self-attention\n",
    "        causality_mask = np.tril(np.ones((1, 1, seq_len, seq_len), dtype=np.int))\n",
    "        attn_mask = torch.from_numpy(causality_mask).to(self.device)\n",
    " \n",
    "        for block in self.transformer_block:\n",
    "            his_vectors = block(his_vectors, attn_mask)\n",
    "        his_vectors = his_vectors * valid_his[:, :, None].float()\n",
    "\n",
    "        his_vector = his_vectors[torch.arange(batch_size), lengths - 1, :]\n",
    "        \n",
    "        \n",
    "        his_vector1 = self.out_it(his_vector)\n",
    "        his_vector2 = self.out_du(his_vector)\n",
    "        his_vector3 = self.out_int(his_vector)\n",
    "       \n",
    "\n",
    "\n",
    "        return his_vector1,his_vector2,his_vector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1f277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfed031",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size, history_max, num_layers, num_heads, item_num,duration_num,interval_num, dropout = 256, 100, 1, 1, 16440,95,21, 0.01\n",
    "model = SASRec(emb_size, history_max, num_layers, num_heads, item_num,duration_num,interval_num, dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09caacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定訓練步驟\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.99), eps=1e-8)\n",
    "epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "logs = []\n",
    "prod_all = []\n",
    "label_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249103d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "for epoch in range(epochs):\n",
    "    co = 0\n",
    "\n",
    "    correct_duration=0\n",
    "    correct_interval=0\n",
    "  \n",
    "    train_loss_sum = 0.0\n",
    "    \n",
    "    train_loss = list()\n",
    "\n",
    "    start_time = timer()\n",
    "    model.train()\n",
    "   \n",
    "    b = []\n",
    "    m, t= 0,0\n",
    "    re3_it, re5_it, re10_it,  ra_it = 0, 0, 0, 0\n",
    "    re2_du, re3_du, re5_du,  ra_du = 0, 0, 0, 0\n",
    "    re2_int, re3_int, re5_int,  ra_int = 0, 0, 0, 0\n",
    "    m3_it,m5_it,m10_it=0,0,0\n",
    "    nd3_it,nd5_it,nd10_it=0,0,0\n",
    "    \n",
    "    m2_du,m3_du,m5_du=0,0,0\n",
    "    nd2_du,nd3_du,nd5_du=0,0,0\n",
    "    \n",
    "    m2_int,m3_int,m5_int=0,0,0\n",
    "    nd2_int,nd3_int,nd5_int=0,0,0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch, data in enumerate(train_loader):\n",
    "        #將input的tuple分成三個部分\n",
    "        \n",
    "        input = { 'item': data[0].to(device),\n",
    "                  'target_item': data[1].to(device),\n",
    "                  'duration': data[2].to(device),\n",
    "                  'target_duration': data[3].to(device),\n",
    "                  'interval': data[4].to(device),\n",
    "                  'target_interval': data[5].to(device),\n",
    "                  'lengths': data[6].to(device),\n",
    "                 }\n",
    "        logits_it,logits_du,logits_int = model(input)\n",
    "        l_it = logits_it\n",
    "        l_du = logits_du\n",
    "        l_int = logits_int\n",
    "        tgt_it = input['target_item']\n",
    "        tgt_du = input['target_duration']\n",
    "        tgt_int = input['target_interval']\n",
    "        \n",
    "       \n",
    "        # CROSS比較train跟target\n",
    "        loss_it = loss_fn(l_it, tgt_it)\n",
    "        loss_du = loss_fn(l_du, tgt_du)\n",
    "        loss_int = loss_fn(l_int, tgt_int)\n",
    "        \n",
    "        loss = 0.6*loss_it + 0.2*loss_du +0.2*loss_int\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.item()\n",
    "       \n",
    "        \n",
    "        \n",
    " \n",
    "    train_loss = train_loss_sum / len(train_loader)\n",
    "    train_time = round(timer() - start_time)\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    import heapq\n",
    "    \n",
    "   \n",
    "\n",
    "    for batch, data in tqdm(enumerate(test_loader)):   \n",
    "        input = { 'item': data[0].to(device),\n",
    "                  'target_item': data[1].to(device),\n",
    "                  'duration': data[2].to(device),\n",
    "                  'target_duration': data[3].to(device),\n",
    "                  'interval': data[4].to(device),\n",
    "                  'target_interval': data[5].to(device),\n",
    "                  'lengths': data[6].to(device),\n",
    "                 }\n",
    "\n",
    "        logits_it,logits_du,logits_int = model(input)\n",
    "   \n",
    "        l_it = logits_it\n",
    "        l_du = logits_du\n",
    "        l_int = logits_int\n",
    "        tgt_it = input['target_item']\n",
    "        tgt_du = input['target_duration']\n",
    "        tgt_int = input['target_interval']\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        l_it =  l_it.cpu().data.numpy()\n",
    "        l_du =  l_du.cpu().data.numpy()\n",
    "        l_int =  l_int.cpu().data.numpy()\n",
    "        ranks_it = 0\n",
    "        ranks3_it = 0\n",
    "        ranks5_it = 0\n",
    "        ranks10_it = 0\n",
    "        ndcg3_it=0\n",
    "        ndcg5_it=0\n",
    "        ndcg10_it=0\n",
    "        \n",
    "        ranks_du = 0\n",
    "        ranks2_du = 0\n",
    "        ranks3_du = 0\n",
    "        ranks5_du = 0\n",
    "        ndcg2_du=0\n",
    "        ndcg3_du=0\n",
    "        ndcg5_du=0\n",
    "        \n",
    "        ranks_int = 0\n",
    "        ranks2_int = 0\n",
    "        ranks3_int = 0\n",
    "        ranks5_int = 0\n",
    "        ndcg2_int=0\n",
    "        ndcg3_int=0\n",
    "        ndcg5_int=0\n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "        #####item\n",
    "        # 將前k個結果取出做比較\n",
    "        for n in range(len(l_it)):\n",
    "             \n",
    "            r3= heapq.nlargest(3, range(len(l_it[n])), l_it[n].take) \n",
    "\n",
    "            \n",
    "            for i in range(len(r3)):\n",
    "                re3_it += ((r3[i]) == tgt_it[n]).float().sum()\n",
    "                \n",
    "            r5= heapq.nlargest(5, range(len(l_it[n])), l_it[n].take)\n",
    "       \n",
    "            for i in range(len(r5)):\n",
    "                re5_it += ((r5[i]) == tgt_it[n]).float().sum()\n",
    "                \n",
    "                \n",
    "            r10= heapq.nlargest(10, range(len(l_it[n])), l_it[n].take)\n",
    "          \n",
    "            for i in range(len(r10)):\n",
    "                re10_it += ((r10[i]) ==tgt_it[n]).float().sum()\n",
    "            \n",
    "          #MRR3\n",
    "            mr3 = r3\n",
    "            for i in range(len(mr3)):\n",
    "                if (mr3[i]) == tgt_it[n]:\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "\n",
    "                    m3_it += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_it+=nndcg3\n",
    "        #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "               \n",
    "                if (mr5[i]) == tgt_it[n]:\n",
    "                    rank5 = i + 1\n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_it += rranks5\n",
    "                    nndcg5=(1/(np.log2(rank5+1)))\n",
    "                    nd5_it+=nndcg5\n",
    "    \n",
    "    \n",
    "    #MRR\n",
    "            mr10 = r10\n",
    "\n",
    "            for i in range(len(mr10)):\n",
    "                if (mr10[i]) == tgt_it[n]:\n",
    "                    rank10 = i + 1\n",
    "                   \n",
    "                    rranks10 = 1/rank10\n",
    "                    m10_it += rranks10\n",
    "                    nndcg10=1/(np.log2(rank10+1))\n",
    "                    nd10_it+=nndcg10\n",
    "        \n",
    "         #####duration\n",
    "        # 將前k個結果取出做比較      \n",
    "        \n",
    "        \n",
    "        for n in range(len(l_du)):\n",
    "            \n",
    "            r1= heapq.nlargest(1,  range(len(l_du[n])), l_du[n].take) \n",
    "            \n",
    "            \n",
    "            for i in range(len(r1)):\n",
    "                correct_duration += (((r1[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r2= heapq.nlargest(2,  range(len(l_du[n])), l_du[n].take) \n",
    "                \n",
    "      \n",
    "            for i in range(len(r2)):\n",
    "                re2_du += (((r2[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r3= heapq.nlargest(3,  range(len(l_du[n])), l_du[n].take) \n",
    "         \n",
    "            for i in range(len(r3)):\n",
    "                re3_du += (((r3[i]) == tgt_du[n]).float().sum())\n",
    "            \n",
    "            r5= heapq.nlargest(5,  range(len(l_du[n])), l_du[n].take) \n",
    "            \n",
    "            for i in range(len(r5)):\n",
    "                re5_du += (((r5[i]) ==tgt_du[n]).float().sum())\n",
    "                \n",
    "           \n",
    "                           \n",
    "          #MRR3\n",
    "            mr2 = r2\n",
    "            for i in range(len(mr2)):\n",
    "                if ((mr2[i]) == tgt_du[n]):\n",
    "                    rank2 = i + 1\n",
    "                    rranks2 = 1/rank2\n",
    "\n",
    "                    m2_du += rranks2\n",
    "                    nndcg2=(1/(np.log2(rank2+1)))\n",
    "                    nd2_du+=nndcg2\n",
    "        #MRR\n",
    "            mr3 = r3\n",
    "\n",
    "            for i in range(len(mr3)):\n",
    "               \n",
    "                if ((mr3[i]) == tgt_du[n]):\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "                    \n",
    "                    m3_du += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_du+=nndcg3\n",
    "    \n",
    "\n",
    "    #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "                if ((mr5[i]) == tgt_du[n]):\n",
    "                    rank5 = i + 1\n",
    "                   \n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_du += rranks5\n",
    "                    nndcg5=1/(np.log2(rank5+1))\n",
    "                    nd5_du+=nndcg5\n",
    "       \n",
    "                    \n",
    "                      \n",
    "       \n",
    "           #####interval\n",
    "        # 將前k個結果取出做比較  \n",
    "        for n in range(len(l_int)):\n",
    "            \n",
    "            \n",
    "            r1= heapq.nlargest(1,  range(len(l_int[n])), l_int[n].take) \n",
    "           \n",
    "            for i in range(len(r1)):\n",
    "                correct_interval += (((r1[i]) == tgt_int[n]).float().sum())\n",
    "            \n",
    "            \n",
    "            r2= heapq.nlargest(2,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "            \n",
    "            for i in range(len(r2)):\n",
    "                re2_int += (((r2[i])) == tgt_int[n]).float().sum()\n",
    "                \n",
    "            r3= heapq.nlargest(3,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "            for i in range(len(r3)):\n",
    "                re3_int += (((r3[i])) == tgt_int[n]).float().sum()\n",
    "                \n",
    "            r5= heapq.nlargest(5,  range(len(l_int[n])), l_int[n].take) \n",
    "            \n",
    "     \n",
    "            for i in range(len(r5)):\n",
    "                re5_int += (((r5[i])) == tgt_int[n]).float().sum()\n",
    "            \n",
    "          #MRR3\n",
    "            mr2 = r2\n",
    "            for i in range(len(mr2)):\n",
    "                if (((mr2[i])) == tgt_int[n]):\n",
    "                    rank2 = i + 1\n",
    "                    rranks2 = 1/rank2\n",
    "\n",
    "                    m2_int += rranks2\n",
    "                    nndcg2=(1/(np.log2(rank2+1)))\n",
    "                    nd2_int+=nndcg2\n",
    "        #MRR\n",
    "            mr3 = r3\n",
    "\n",
    "            for i in range(len(mr3)):\n",
    "               \n",
    "                if ((mr3[i])) == tgt_int[n]:\n",
    "                    rank3 = i + 1\n",
    "                    rranks3 = 1/rank3\n",
    "                    m3_int += rranks3\n",
    "                    nndcg3=(1/(np.log2(rank3+1)))\n",
    "                    nd3_int+=nndcg3\n",
    "    \n",
    " \n",
    "    #MRR\n",
    "            mr5 = r5\n",
    "\n",
    "            for i in range(len(mr5)):\n",
    "                if ((mr5[i])) == tgt_int[n]:\n",
    "                    rank5 = i + 1\n",
    "                   \n",
    "                    rranks5 = 1/rank5\n",
    "                    m5_int += rranks5\n",
    "                    nndcg5 = 1/(np.log2(rank5+1))\n",
    "                    nd5_int+=nndcg5  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "   \n",
    "    \n",
    "    recall3_it = (re3_it*100)/(len(test_data[0]))\n",
    "    \n",
    "    recall5_it = (re5_it*100)/(len(test_data[0]))\n",
    "    recall10_it = (re10_it*100)/(len(test_data[0]))\n",
    "    mrr3_it = (m3_it*100)/(len(test_data[0]))\n",
    "    mrr5_it = (m5_it*100)/(len(test_data[0]))\n",
    "    mrr10_it = (m10_it*100)/(len(test_data[0]))\n",
    "    n3_it=(nd3_it*100)/(len(test_data[0]))\n",
    "    n5_it=(nd5_it*100)/(len(test_data[0]))\n",
    "    n10_it=(nd10_it*100)/(len(test_data[0]))\n",
    "   \n",
    "        \n",
    "        \n",
    "    recall2_du = (re2_du*100)/(len(test_data[0]))\n",
    "    recall3_du = (re3_du*100)/(len(test_data[0]))\n",
    "    recall5_du = (re5_du*100)/(len(test_data[0]))\n",
    "    mrr2_du = (m2_du*100)/(len(test_data[0]))\n",
    "    mrr3_du = (m3_du*100)/(len(test_data[0]))\n",
    "    mrr5_du = (m5_du*100)/(len(test_data[0]))\n",
    "    n2_du=(nd2_du*100)/(len(test_data[0]))\n",
    "    n3_du=(nd3_du*100)/(len(test_data[0]))\n",
    "    n5_du=(nd5_du*100)/(len(test_data[0]))\n",
    "    \n",
    "        \n",
    "        \n",
    "    recall2_int = (re2_int*100)/(len(test_data[0]))\n",
    "    recall3_int = (re3_int*100)/(len(test_data[0]))\n",
    "    recall5_int = (re5_int*100)/(len(test_data[0]))\n",
    "    mrr2_int = (m2_int*100)/(len(test_data[0]))\n",
    "    mrr3_int = (m3_int*100)/(len(test_data[0]))\n",
    "    mrr5_int = (m5_int*100)/(len(test_data[0]))\n",
    "    n2_int=(nd2_int*100)/(len(test_data[0]))\n",
    "    n3_int=(nd3_int*100)/(len(test_data[0]))\n",
    "    n5_int=(nd5_int*100)/(len(test_data[0]))\n",
    "\n",
    "        \n",
    "    accuracy_du = 100 * correct_duration/(len(test_data[0]))\n",
    "    accuracy_int = 100 * correct_interval/(len(test_data[0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "    log = f\"Accuracy_Duration: {accuracy_du:.5f} Accuracy_Interval: {accuracy_int:.5f}  HR3_Item: {recall3_it:.5f} HR5_Item: {recall5_it:.5f} HR10_Item: {recall10_it:.5f}  HR2_du: {recall2_du:.5f} HR3_du: {recall3_du:.5f} HR5_du: {recall5_du:.5f} HR2_int: {recall2_int:.5f} HR3_int: {recall3_int:.5f} HR5_int: {recall5_int:.5f}  MRR3_item: {mrr3_it:.10f}  MRR5_item: {mrr5_it:.10f}  MRR10_item: {mrr10_it:.10f} MRR2_du: {mrr2_du:.10f}  MRR3_du: {mrr3_du:.10f}  MRR5_du: {mrr5_du:.10f} MRR2_int: {mrr2_int:.10f}  MRR3_int: {mrr3_int:.10f}  MRR5_int: {mrr5_int:.10f}  NDCG3_it: {n3_it:.10f}  NDCG5_it: {n5_it:.10f}  NDCG10_it: {n10_it:.10f}  NDCG2_du: {n2_du:.10f}  NDCG3_du: {n3_du:.10f}  NDCG5_du: {n5_du:.10f}  NDCG2_int: {n2_int:.10f}  NDCG3_int: {n3_int:.10f}  NDCG5_int: {n5_int:.10f} \"\n",
    "        \n",
    "    print(\"train_loss_sum:\",train_loss_sum) \n",
    "    print(log)\n",
    "    \n",
    "   # print(m/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bea110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e1363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheng",
   "language": "python",
   "name": "cheng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
